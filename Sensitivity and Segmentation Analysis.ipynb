{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5d8bca",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime as dt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad217f1a",
   "metadata": {},
   "source": [
    "## 2. Input files (Spend, Name normalizer, Risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a39ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.10f' % x)\n",
    "spend_input = pd.read_csv('C:/Users/mpadmara/FinalSpend.csv')  #Load the spend file in the desired format as mentioned in the SOP\n",
    "Normalizer = pd.read_csv(\"C:/Users/mpadmara/Downloads/Name Normalization Input.csv\")  # Load the Name Normalization file in the desired format as mentioned in the SOP\n",
    "risk_user_input = pd.read_excel(\"C:/Users/mpadmara/Downloads/Risk Input.xlsx\")  # Load the risk input file in the desired format as mentioned in the SOP\n",
    "spend_input = spend_input[spend_input.columns[spend_input.columns.isin(\n",
    "    ['amount', 'business_country', 'business_region', 'category''l1_category', 'l2_category',\n",
    "     'Months in date', 'category', 'department', 'id', 'is_preferred', 'is_diverse', 'is_managed',\n",
    "     'normalized_vendor_name', 'original_amount', 'original_currency', 'original_vendor_name', 'subcategory'])]] # Select the mentioned columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39ee457",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138be4a4",
   "metadata": {},
   "source": [
    "### 3.1 Transforming amount values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_input['amount'] = spend_input['amount'].astype(str)\n",
    "spend_input['original_amount'] = spend_input['original_amount'].astype(str)\n",
    "spend_input['amount'] = spend_input.amount.str.split(',').str.join('')\n",
    "spend_input['original_amount'] = spend_input.original_amount.str.split(',').str.join('')\n",
    "spend_input['amount'] = pd.to_numeric(spend_input['amount'], errors='coerce')\n",
    "spend_input['original_amount'] = pd.to_numeric(spend_input['original_amount'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e82692a",
   "metadata": {},
   "source": [
    "### 3.2 Replace Not Provided to Null for business country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47120d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_input['business_country'].replace('NOT PROVIDED', np.NaN,\n",
    "                                        inplace=True)  # Replace Not Provided with null for future imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4217e6cd",
   "metadata": {},
   "source": [
    "### 3.3 Impute missing Business Regions based on currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efdc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_input['business_region'] = \\\n",
    "spend_input.groupby('original_currency').transform(lambda x: x.fillna(x.value_counts().index[0]))[\n",
    "    'business_region']  # impute business region based on currency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c0673",
   "metadata": {},
   "source": [
    "### 3.4 Impute missing Business Country based on business region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8946c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_input['business_country'] = \\\n",
    "spend_input.groupby('business_region').transform(lambda x: x.fillna(x.value_counts().index[0]))[\n",
    "    'business_country']  # impute country based on region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff46a81",
   "metadata": {},
   "source": [
    "### 3.5 Impute missing Department based on category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_input['department'] = \\\n",
    "spend_input.groupby('l2_category').transform(lambda x: x.fillna(x.value_counts().index[0]))[\n",
    "    'department']  # impute department based on category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3d69a",
   "metadata": {},
   "source": [
    "### 3.6 Replace cities and acronynms with actual Business Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9175284",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_replace = {'GBR': 'GREAT BRITAIN', 'USA': 'UNITED STATES', 'US': 'UNITED STATES', 'HONG KONG': 'CHINA',\n",
    "                   'BERMUDA': 'GREAT BRITAIN', 'MIAMI': 'UNITED STATES', 'TAIWAN': 'CHINA',\n",
    "                   'PUERTO RICO': 'UNITED STATES', 'MACAU': 'CHINA'}\n",
    "spend_input['business_country'].replace(to_replace=vals_to_replace, inplace=True)  # Correct country mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482cb53",
   "metadata": {},
   "source": [
    "### 3.7 Identify and remove erroneous vendor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "list_of_vendors_as_strings = [str(i) for i in set(list(spend_input['normalized_vendor_name']))]\n",
    "vendor_list = [i for i in list_of_vendors_as_strings]\n",
    "isenglish = [isEnglish(i) for i in list_of_vendors_as_strings]\n",
    "vendor_df = pd.DataFrame(list(zip(vendor_list, isenglish)), columns=['normalized_vendor_name', 'flag'])\n",
    "\n",
    "spend_input_vendorflag = spend_input.merge(vendor_df, how='left', on=['normalized_vendor_name'])\n",
    "spend_input_updated_vendors = spend_input_vendorflag[spend_input_vendorflag['flag'] == True]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b217ca0",
   "metadata": {},
   "source": [
    "### 3.8 Remove claims related expenses and other errorneous vendors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_input_remove_claimexpenses = spend_input_updated_vendors[\n",
    "    spend_input_updated_vendors['subcategory'] != 'CLAIMS RELATED EXPENSES']\n",
    "filter_list = ['00000', '00', '02 P', '1', '0001', '2', '3', '4', '004', '5', '6', '7', '8', '9', '10', '11', '13',\n",
    "               '00014', '16', '17', '18', '21', '24', '25', '26', '30', '42', '43', '44', '45', '46', '49', '50', '58',\n",
    "               '66', '67', '81', '82', '83', '95', '99', '105', '111', '146', '165', '170', '205', '220', '241',\n",
    "               '00358', '379', '401', '402', '407', '428', '484', '557', '00610', '00611', '616', '618', '638', '711',\n",
    "               '0000000724', '759', '825', '857', '1094', '0000001140', '0000001143', '1212', '1250', '1268', '1418',\n",
    "               '1424', '1444', '1720', '1741', '1760', '1810', '1823', '1826', '1831', '1852', '1862',\n",
    "               '1880', '1908', '1909', '1919', '1920', '1933', '1935', '1951', '1957', '1962', '1965', '2127', '2248',\n",
    "               '3106', '3412', '3414', '3416', '3531', '3564', '4278', '4760', '5050', '6207', '0000006291',\n",
    "               '0007739', '12306', '16122', '24166', '30113', '34739', '35886', '37965', '41100', '41871', '41874',\n",
    "               '7/9/2019', '11/16/19', '10/6/2020', '44193', 'MAY 20', 'AUG 1', 'OCTOBER28', '51165', '51450',\n",
    "               '51769', '59941', '60111', '61255', '69482', '72109', '73310', '73460', '73748', '73769', '73882',\n",
    "               '73935', '74019', '74024', '74067', '74106', '74387', '74416', '74561', '75143', '75145', '75383',\n",
    "               '75413', '75992', '76020', '76406', '76409', '76684', '820085', '1034260', '1066147', '1091540',\n",
    "               '1095050', '1095704', '1635964', '5985346', '10685122', '460775834', '845188728', '6042410000',\n",
    "               '73690600634', '00720000290562', '2000011497406', '5E73', '#120', ',', '..', '...', '......',\n",
    "               '?????????', '+++++++', 'N A', 'NO VENDOR NAME PROVIDED', 'NOT', 'NOT APPLICABLE']\n",
    "spend_input_filtered = spend_input_remove_claimexpenses[\n",
    "    spend_input_remove_claimexpenses.normalized_vendor_name.isin(filter_list) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ab667",
   "metadata": {},
   "source": [
    "### 3.9 Perform vendor name normalization based on normalization input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e84d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_input_name_normalization = spend_input_filtered.merge(Normalizer, how='left',\n",
    "                                                            on=['normalized_vendor_name', 'category'])\n",
    "spend_input_post_namenormalization = spend_input_name_normalization\n",
    "spend_input_post_namenormalization['normalized_vendor_name'] = np.where(\n",
    "    spend_input_post_namenormalization['Normalized'].isnull(),\n",
    "    spend_input_post_namenormalization['normalized_vendor_name'], spend_input_post_namenormalization['Normalized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea68d01",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04b19b",
   "metadata": {},
   "source": [
    "### 4.1 Model Inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_assignment = pd.DataFrame()\n",
    "dates = []\n",
    "spend_input_post_namenormalization[\"Months in date\"] = pd.to_datetime(\n",
    "    spend_input_post_namenormalization[\"Months in date\"])\n",
    "spend_input_post_namenormalization['months'] = spend_input_post_namenormalization['Months in date'].dt.to_period('m')\n",
    "end_month = spend_input_post_namenormalization['months'].max()\n",
    "start_month = end_month - 12\n",
    "while start_month <= end_month:\n",
    "    dates.append(start_month.to_timestamp(freq='M'))\n",
    "    start_month += 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9ad93",
   "metadata": {},
   "source": [
    "### 4.1.1 Input spend weights (0-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_weight = []\n",
    "n = int(input(\"Enter the number of weights  \"))\n",
    "print(\"\\n\")\n",
    "for i in range(0, n):\n",
    "    print(\"Enter the weight at index\", i, \"from 0-100\" )\n",
    "    item = int(input())\n",
    "    spend_weight.append(item)\n",
    "print(spend_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e75aa",
   "metadata": {},
   "source": [
    "### 4.1.2 Input Pareto contribution percent (0-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f739d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_input = []\n",
    "n = int(input(\"Enter the number of iterations for pareto\"))\n",
    "print(\"\\n\")\n",
    "for i in range(0, n):\n",
    "    print(\"Enter number at index\", i,\"from 0-100\" )\n",
    "    item = int(input())\n",
    "    percent_input.append(item)\n",
    "print(percent_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a280195",
   "metadata": {},
   "source": [
    "### 4.1.3 Input recency of data in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98342b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_input = []\n",
    "n = int(input(\"Enter the number of months for recency\"))\n",
    "print(\"\\n\")\n",
    "for i in range(0, n):\n",
    "    print(\"Enter number at index\", i,\"(in days)\" )\n",
    "    item = int(input())\n",
    "    days_input.append(item)\n",
    "print(days_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84137b82",
   "metadata": {},
   "source": [
    "### 4.2 Clustering of vendors based on spend and risk for all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb52466",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates:\n",
    "    spend_input_post_namenormlaization_quarter = spend_input_post_namenormalization[\n",
    "        spend_input_post_namenormalization['Months in date'] <= date]\n",
    "    spend_input_datesgroup = spend_input_post_namenormlaization_quarter.groupby(['category', 'normalized_vendor_name'])\n",
    "    spend_input_recency = spend_input_datesgroup.agg(minimum_Date=('Months in date', np.min),\n",
    "                                                     maximum_date=('Months in date', np.max))\n",
    "    spend_input_recency['latest_date'] = spend_input_recency['maximum_date'].max()\n",
    "    spend_input_recency['difference_From_latest'] = spend_input_recency['latest_date'] - spend_input_recency[\n",
    "        'maximum_date']\n",
    "    spend_input_recency['difference_From_latest'] = spend_input_recency['difference_From_latest'].dt.days\n",
    "    \n",
    "    for days in days_input:\n",
    "        vendors_latest = spend_input_recency[spend_input_recency['difference_From_latest'] <= days]\n",
    "        vendors_latest.reset_index(inplace=True)\n",
    "        vendors_latest_flagged = spend_input_post_namenormlaization_quarter.merge(\n",
    "            vendors_latest[['category', 'normalized_vendor_name', 'difference_From_latest']], how='left',\n",
    "            on=['category', 'normalized_vendor_name'])\n",
    "        vendors_latest_filtered = vendors_latest_flagged[vendors_latest_flagged['difference_From_latest'].notnull()]\n",
    "        vendors_latest_filtered_grouped = vendors_latest_filtered.groupby(['category', 'normalized_vendor_name']).sum()\n",
    "        vendors_latest_filtered_grouped.reset_index(inplace=True)\n",
    "        vendors_latest_filtered_grouped = vendors_latest_filtered_grouped.rename(\n",
    "            columns={'amount': 'grouped_amount', 'original_amount': 'grouped_original_amount'})\n",
    "        vendors_latest_filtered_totals = vendors_latest_filtered.merge(vendors_latest_filtered_grouped[\n",
    "                                                                           ['category', 'normalized_vendor_name',\n",
    "                                                                            'grouped_amount',\n",
    "                                                                            'grouped_original_amount']], how='left',\n",
    "                                                                       on=['category', 'normalized_vendor_name'])\n",
    "        vendors_latest_filtered_totals['contribution'] = vendors_latest_filtered_totals['grouped_amount'] * 100 / \\\n",
    "                                                         vendors_latest_filtered_totals.groupby(['category'])[\n",
    "                                                             'grouped_amount'].transform('sum')\n",
    "        vendors_latest_filtered_totals_sorted = vendors_latest_filtered_totals.groupby('category').apply(\n",
    "            lambda x: x.sort_values(by='contribution', ascending=False).reset_index(drop=True))\n",
    "        subcat_analysis = vendors_latest_filtered_totals_sorted.drop('category', axis=1).reset_index()\n",
    "        vendors_latest_filtered_totals_sorted_unique = subcat_analysis[\n",
    "            ['category', 'normalized_vendor_name', 'grouped_amount']].drop_duplicates()\n",
    "        vendors_latest_filtered_totals_sorted_unique['contribution'] = vendors_latest_filtered_totals_sorted_unique[\n",
    "                                                                           'grouped_amount'] * 100 / \\\n",
    "                                                                       vendors_latest_filtered_totals_sorted_unique.groupby(\n",
    "                                                                           ['category'])['grouped_amount'].transform(\n",
    "                                                                           'sum')\n",
    "        vendors_latest_filtered_totals_sorted_unique['cumsum'] = \\\n",
    "        vendors_latest_filtered_totals_sorted_unique.groupby('category')['contribution'].transform('cumsum')\n",
    "        \n",
    "        for percent in percent_input:\n",
    "            pareto = vendors_latest_filtered_totals_sorted_unique[\n",
    "                vendors_latest_filtered_totals_sorted_unique['cumsum'] <= percent]\n",
    "            pareto_subcat = pareto.merge(subcat_analysis[['category', 'normalized_vendor_name', 'subcategory']],\n",
    "                                         on=['category', 'normalized_vendor_name'], how='left').drop_duplicates()\n",
    "            subcat = set(list(pareto_subcat['subcategory']))\n",
    "            subcat_exhaustive = set(list(subcat_analysis['subcategory']))\n",
    "            missing_subcat = list(subcat_exhaustive - subcat)\n",
    "            missingsubcat_df = subcat_analysis[subcat_analysis['subcategory'].isin(missing_subcat)]\n",
    "            missingsubcat_grouped = missingsubcat_df.groupby(\n",
    "                ['category', 'normalized_vendor_name', 'subcategory']).sum()\n",
    "            missingsubcat_grouped.reset_index(inplace=True)\n",
    "            appendmissing_subcat = missingsubcat_grouped[\n",
    "                missingsubcat_grouped['amount'] == missingsubcat_grouped.groupby('subcategory')['amount'].transform(\n",
    "                    'max')][['category', 'normalized_vendor_name', 'amount']]\n",
    "            appendmissing_subcat = appendmissing_subcat.rename(columns={'amount': 'grouped_amount'})\n",
    "            spend = pd.concat([pareto, appendmissing_subcat], axis=0)\n",
    "            risk_input = pd.DataFrame(list(spend['normalized_vendor_name'].drop_duplicates()),\n",
    "                                      columns=['normalized_vendor_name'])\n",
    "            risk = risk_input.merge(risk_user_input, on='normalized_vendor_name', how='left')\n",
    "            random.seed(10)\n",
    "            risk['Cyber Risk Score input'] = [random.randint(1, 6) for k in risk.index]\n",
    "            risk['Resiliency Risk Score input'] = [random.randint(1, 6) for k in risk.index]\n",
    "            risk['Compliance Risk Score input'] = [random.randint(1, 6) for k in risk.index]\n",
    "            risk['Data Privacy Risk Score input'] = [random.randint(1, 6) for k in risk.index]\n",
    "            risk['Financial Risk Score input'] = [random.randint(1, 6) for k in risk.index]\n",
    "            risk['ESG input'] = [random.randint(1, 6) for k in risk.index]\n",
    "            risk['Cyber Risk Score'].fillna(risk['Cyber Risk Score input'], inplace=True)\n",
    "            risk['Resiliency Risk Score'].fillna(risk['Resiliency Risk Score input'], inplace=True)\n",
    "            risk['Compliance Risk Score'].fillna(risk['Compliance Risk Score input'], inplace=True)\n",
    "            risk['Data Privacy Risk Score'].fillna(risk['Data Privacy Risk Score input'], inplace=True)\n",
    "            risk['Financial Risk Score'].fillna(risk['Financial Risk Score input'], inplace=True)\n",
    "            risk['ESG'].fillna(risk['ESG input'], inplace=True)\n",
    "            clustering_data = spend.merge(risk, on='normalized_vendor_name', how='left')\n",
    "            col_list = ['Cyber Risk Score', 'Resiliency Risk Score', 'Compliance Risk Score', 'Data Privacy Risk Score',\n",
    "                        'Financial Risk Score', 'ESG']\n",
    "            clustering_data['Totalrisk'] = clustering_data[col_list].sum(axis=1)\n",
    "            clustering_data['Norisk'] = 10 * len(col_list) - clustering_data['Totalrisk']\n",
    "            clustering_data['norisk_contribution_overall'] = clustering_data['Norisk'] / clustering_data['Norisk'].sum()\n",
    "            clustering_data['spend_contribution_overall'] = clustering_data['grouped_amount'] / clustering_data[\n",
    "                'grouped_amount'].sum()\n",
    "            clustering_data['risk_contribution_overall'] = clustering_data['Totalrisk'] / clustering_data[\n",
    "                'Totalrisk'].sum()\n",
    "\n",
    "            for i in spend_weight:\n",
    "                clustering_data['objective_spend_overall'] = (clustering_data['spend_contribution_overall']) * i\n",
    "                if i == 100:\n",
    "                    clustering_data['objective_norisk_overall'] = 0\n",
    "                else:\n",
    "                    clustering_data['objective_norisk_overall'] = clustering_data['norisk_contribution_overall'] * (\n",
    "                            100 - i)\n",
    "                IQR = clustering_data['objective_spend_overall'].quantile(0.75) - clustering_data[\n",
    "                    'objective_spend_overall'].quantile(0.25)\n",
    "                median = clustering_data['objective_spend_overall'].median()\n",
    "                # clustering_data_without_outliers=clustering_data.loc[clustering_data['objective_spend_overall'] < clustering_data['objective_spend_overall'].quantile(0.99)]\n",
    "                df_cons = clustering_data.sort_values(by='objective_spend_overall', ascending=False)\n",
    "                clustering_data_without_outliers = df_cons.iloc[50:]\n",
    "                X = clustering_data_without_outliers.loc[:,\n",
    "                    ['objective_spend_overall', 'objective_norisk_overall']].values\n",
    "                gmm = GaussianMixture(n_components=4,random_state=42)\n",
    "                gmm.fit(X)\n",
    "                labels = gmm.predict(X)\n",
    "                clustering_data_without_outliers['Gaussian'] = pd.Series(labels,\n",
    "                                                                         index=clustering_data_without_outliers.index)\n",
    "                list_labels = list(labels)\n",
    "                count = [list_labels.count(i) for i in set(list_labels)]\n",
    "                count_df = pd.DataFrame(count, columns=['Count'])\n",
    "                # append=clustering_data.loc[clustering_data['objective_spend_overall'] >= clustering_data['objective_spend_overall'].quantile(0.99)]\n",
    "                append = clustering_data.nlargest(50, ['objective_spend_overall'])\n",
    "                append['Gaussian'] = count_df['Count'].idxmin()\n",
    "                cluster_assignment = pd.concat([clustering_data_without_outliers, append], axis=0)\n",
    "                cluster_assignment['SpendWeight'] = i\n",
    "                cluster_assignment['Pareto'] = percent\n",
    "                cluster_assignment['recency'] = days\n",
    "                cluster_assignment['Quarter'] = date\n",
    "                final_assignment = final_assignment.append(cluster_assignment, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f425ae",
   "metadata": {},
   "source": [
    "## 5. Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d851f24",
   "metadata": {},
   "source": [
    "### 5.1 Generate Tiers based on modeling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852843f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ranks that indicate tiers based on count for each gaussian label\n",
    "final_assignment['counts'] = final_assignment.groupby(['Quarter','Gaussian','Pareto','recency','SpendWeight'])['SpendWeight'].transform('count')\n",
    "final_assignment['Rank'] = final_assignment.groupby(['Quarter','Pareto','recency','SpendWeight'])['counts'].rank(method='dense').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5651264",
   "metadata": {},
   "source": [
    "### 5.2 Output the result to populate dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e991d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard=final_assignment.merge(spend_input_post_namenormalization[['business_country','business_region','category','normalized_vendor_name','subcategory','is_preferred','is_diverse','is_managed']].drop_duplicates(),on=['category','normalized_vendor_name'],how='left').drop_duplicates()\n",
    "dashboard.to_csv('DashboardAnalysis.csv')  # Check working directory for the ouput file\n",
    "spend_input_post_namenormalization.to_csv('ClusterInput.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
